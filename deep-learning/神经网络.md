## 从感知机到神经网络
神经网络举例

[![Ekjvid.jpg](https://s2.ax1x.com/2019/04/22/Ekjvid.jpg)](https://imgchr.com/i/Ekjvid)

带激活函数h(x)的神经元

[![EkvJY9.jpg](https://s2.ax1x.com/2019/04/22/EkvJY9.jpg)](https://imgchr.com/i/EkvJY9)

### sigmoid函数
**sigmoid函数**

[![Ekv6fA.jpg](https://s2.ax1x.com/2019/04/22/Ekv6fA.jpg)](https://imgchr.com/i/Ekv6fA)

绘制sigmoid函数
```python
import numpy as np
import matplotlib.pyplot as plt

x = np.arange(-10, 10, 0.1)
y = np.abs(1 / (1 + np.exp(-x)))

plt.plot(x, y)
plt.show()
```

![Ekvv0U.jpg](https://s2.ax1x.com/2019/04/22/Ekvv0U.jpg)


### 阶跃函数
```python
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x > 0, dtype = np.int)

x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
# 指定y轴的范围
plt.ylim(-0.1, 1.1)
plt.show()
```

[![Ekx0cq.jpg](https://s2.ax1x.com/2019/04/22/Ekx0cq.jpg)](https://imgchr.com/i/Ekx0cq)

### 非线性函数

sigmoid函数和阶跃函数一样都是**非线性函数**

线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。为了具体地（稍微直观地）理解这一点，我们来思考下面这个简单的例子。这里我们考虑把线性函数`h(x) = cx`作为激活函数，把`y(x) = h(h(h(x)))`的运算对应3层神经网络A 。这个运算会进行`y(x) = c × c × c × x`的乘法运算，但是同样的处理可以由`y(x) = ax`（注意，`a = c^3` ）这一次乘法运算（即没有隐藏层的神经网络）来表示。如本例所示，使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数

### ReLU函数
![EAEVFx.jpg](https://s2.ax1x.com/2019/04/22/EAEVFx.jpg)

```python
import numpy as np
import matplotlib.pylab as plt

def relu(x):
    return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.show()
```

![EAEZY6.jpg](https://s2.ax1x.com/2019/04/22/EAEZY6.jpg)

## 多维数组
数组的维数可以通过 `np.dim()` 函数获得
数组的形状
可以通过实例变量 `shape` 获得

```
B = np.array([[1,2],[3,4],[5,6]])
print(B)
print(np.ndim(B))
print(B.shape)

输出结果：
[[1 2]
 [3 4]
 [5 6]]
2
(3, 2)
```
np.dot() 接收两个NumPy数组作为参
数，并返回矩阵的乘积
```
B = np.array([[1,2],[3,4]])
A = B
print(np.dot(A, B))

输出结果：
[[ 7 10]
 [15 22]]
```

<!-- <center> -->
[![EAMtyV.jpg](https://s2.ax1x.com/2019/04/22/EAMtyV.jpg)](https://imgchr.com/i/EAMtyV)
<!-- </center> -->

<center>神经网络识别相应节点图<center>

